---
---


@inproceedings{SoftBioMorph,
  bibtex_show={true},
  author = {Nicolae, Madalina and Lefez, Claire and Roudaut, Anne and Huron, Samuel and Steimle, J\"{u}rgen and Teyssier, Marc},
  title = {SoftBioMorph: Fabricating Sustainable Shape-changing Interfaces using Soft Biopolymers},
  year = {2024},
  isbn = {9798400705830},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3643834.3661610},
  doi = {10.1145/3643834.3661610},
  abstract = {Bio-based and bio-degradable materials have shown promising results for sustainable Human-Computer Interaction (HCI) applications, including shape-changing interfaces. However, the diversity of shape-changing behaviors achievable with these materials remains unclear as the fabrication knowledge is scattered across multiple research fields. This paper introduces SoftBioMorph, a fabrication framework that aims to integrate the fabrication know-how of sustainable soft shape-changing interfaces with biopolymers. Based on the example of Sodium Alginate, the framework contributes (1) a set of material synthesis processes that modify the biopolymer’s properties to fulfill different functions; (2) a set of DIY crafting-based assembling techniques that functionalize the material and assembling properties to achieve three primitive types of change in shape; and (3) a series of application cases that demonstrate the versatility of the framework. We further discuss limitations, research questions, and fabrication challenges, presenting a comprehensive approach to sustainable prototyping in HCI.},
  booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
  pages = {496–508},
  numpages = {13},
  keywords = {DIY, biomaterials, bioplastics, biopolymers, fabrication, prototyping, shape-changing interface, sustainability, tangible interfaces},
  location = {Copenhagen, Denmark},
  series = {DIS '24},
  video={https://www.youtube-nocookie.com/embed/HkEnuYZDb9k},
  preview={2024-softbiomorph.png},
  selected={true},
  series = {CHI '24},
  pdf={https://dl.acm.org/doi/abs/10.1145/3643834.3661610}
}

@inproceedings{Bressa2024,
  bibtex_show={true},
  author = {Bressa, Nathalie and Louis, Jordan and Willett, Wesley and Huron, Samuel},
  title = {Input Visualization: Collecting and Modifying Data with Visual Representations},
  year = {2024},
  isbn = {9798400703300},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3613904.3642808},
  doi = {10.1145/3613904.3642808},
  abstract = {We examine input visualizations, visual representations that are designed to collect (and represent) new data rather than encode preexisting datasets. Information visualization is commonly used to reveal insights and stories within existing data. As a result, most contemporary visualization approaches assume existing datasets as the starting point for design, through which that data is mapped to visual encodings. Meanwhile, the implications of visualizations as inputs and as data sources have received little attention—despite the existence of visual and physical examples stretching back centuries. In this paper, we present a design space of 50 input visualizations analyzing their visual representation, data, artifact, context, and input. Based on this, we identify input modalities, purposes of input visualizations, and a set of design considerations. Finally, we discuss the relationship between input visualization and traditional visualization design and suggest opportunities for future research to better understand these visual representations and their potential.},
  booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  articleno = {499},
  numpages = {18},
  keywords = {data collection, data discussion, input visualization, interaction, participation, physicalization, public engagement, survey, visualization},
  location = {Honolulu, HI, USA},
  video={https://www.youtube-nocookie.com/embed/wilUZ5wbTiw},
  preview={2024-chi-table1.png},
  selected={true},
  series = {CHI '24},
  pdf={https://osf.io/c9j8s},
  url={https://osf.io/bw3gp/?view_only=}
}

@inproceedings{Bonnail2024,
  bibtex_show={true},
  author = {Bonnail, Elise and Frommel, Julian and Lecolinet, Eric and Huron, Samuel and Gugenheimer, Jan},
  title = {Was it Real or Virtual? Confirming the Occurrence and Explaining Causes of Memory Source Confusion between Reality and Virtual Reality},
  year = {2024},
  isbn = {9798400703300},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3613904.3641992},
  doi = {10.1145/3613904.3641992},
  abstract = {Source confusion occurs when individuals attribute a memory to the wrong source (e.g., confusing a picture with an experienced event). Virtual Reality (VR) represents a new source of memories particularly prone to being confused with reality. While previous research identified causes of source confusion between reality and other sources (e.g., imagination, pictures), there is currently no understanding of what characteristics specific to VR (e.g., immersion, presence) could influence source confusion. Through a laboratory study (n=29), we 1) confirm the existence of VR source confusion with current technology, and 2) present a quantitative and qualitative exploration of factors influencing VR source confusion. Building on the Source Monitoring Framework, we identify VR characteristics and assumptions about VR capabilities (e.g., poor rendering) that are used to distinguish virtual from real memories. From these insights, we reflect on how the increasing realism of VR could leave users vulnerable to memory errors and perceptual manipulations.},
  booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  articleno = {796},
  numpages = {17},
  keywords = {Memory, Source Confusion, Source Misattribution, Virtual Reality},
  location = {Honolulu, HI, USA},
  video={https://www.youtube-nocookie.com/embed/NagIRCvq-Eo},
  preview={2024-source-confusion.jpg},
  series = {CHI '24}
}

@INPROCEEDINGS{Tseng2024,
  bibtex_show={true},
  author={Tseng, Wen-Jie and Kontrazis, Petros Dimitrios and Lecolinet, Eric and Huron, Samuel and Gugenheimer, Jan},
  booktitle={2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Understanding Interaction and Breakouts of Safety Boundaries in Virtual Reality Through Mixed-Method Studies}, 
  year={2024},
  volume={},
  number={},
  pages={482-492},
  keywords={Surveys;Three-dimensional displays;Virtual reality;User interfaces;Aerospace electronics;Encoding;Safety;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Empirical studies in HCI},
  doi={10.1109/VR58804.2024.00069},
  preview={2024-Breakouts-of-Safety-Boundaries.png}
}

@book{huron2022making,
  bibtex_show={true},
  title={Making with Data: Physical Design and Craft in a Data-Driven World},
  author={Huron, Samuel and Nagel, Till and Oehlberg, Lora and Willett, Wesley},
  year={2022},
  publisher={CRC Press},
  preview={2022-MKWD.jpg},
  abstract = {Making with Data: Physical Design and Craft in a Data-Driven World provides a snapshot of the diverse practices contemporary creators are using to produce objects, spaces, and experiences imbued with data. Across 25+ beautifully-illustrated chapters, international artists, designers, and scientists each explain the process of creating a specific data-driven piece—illustrating their practice with candid sketches, photos, and design artifacts from their own studios.},
  url={https://makingwithdata.org/}
}


@inproceedings{boukhelifa2017data,
  bibtex_show = {true},
  title = {How data workers cope with uncertainty: A task characterisation study},
  author = {Boukhelifa, Nadia and Perrin, Marc-Emmanuel and Huron, Samuel and Eagan, James},
  booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
  pages = {3645--3656},
  pdf = {https://hal.science/hal-01472865},
  video = {https://www.youtube.com/watch?v=lWXBNH_zCEI},
  annotation={* This paper received an Honorable Mention Award},
  preview={2017-characterizing-uncertainty.png},
  year = {2017}
}

@inproceedings{huron2017let,
  bibtex_show={true},
  title={Let's get physical: Promoting data physicalization in workshop formats},
  author={Huron, Samuel and Gourlet, Pauline and Hinrichs, Uta and Hogan, Trevor and Jansen, Yvonne},
  booktitle={Proceedings of the 2017 Conference on Designing Interactive Systems},
  pdf = {https://hal.sorbonne-universite.fr/hal-01538595},
  preview={2017-lets-get-physical.png},
  pages={1409--1422},
  year={2017}
}

@article{thudt2015visual,
  bibtex_show={true},
  title={Visual mementos: Reflecting memories with personal data},
  author={Thudt, Alice and Baur, Dominikus and Huron, Samuel and Carpendale, Sheelagh},
  journal={IEEE transactions on visualization and computer graphics},
  volume={22},
  number={1},
  pages={369--378},
  year={2015},
  preview={2015-visual-mementos.png},
  video={https://vimeo.com/236169885},
  publisher={IEEE}
}

@inproceedings{walny2015exploratory,
  bibtex_show={true},
  title={An exploratory study of data sketching for visual representation},
  abstract={Hand-drawn sketching on napkins or whiteboards is a common, accessible method for generating visual representations. This practice is shared by experts and non-experts and is probably one of the faster and more expressive ways to draft a visual representation of data. In order to better understand the types of and variations in what people produce when sketching data, we conducted a qualitative study. We asked people with varying degrees of visualization expertise, from novices to experts, to manually sketch representations of a small, easily understandable dataset using pencils and paper and to report on what they learned or found interesting about the data. From this study, we extract a data sketching representation continuum from numeracy to abstraction; a data report spectrum from individual data items to speculative data hypothesis; and show the correspondence between the representation types and the data reports from our results set. From these observations we discuss the participants’ representations in relation to their data reports, indicating implications for design and potentially fruitful directions for research.},
  author={Walny, Jagoda and Huron, Samuel and Carpendale, Sheelagh},
  booktitle={Computer Graphics Forum},
  volume={34},
  number={3},
  pages={231--240},
  preview={2015-data-sketching.png},
  pdf={https://inria.hal.science/hal-01024053/document},
  selected={true},
  year={2015}
}

@article{huron2014constructing,
  bibtex_show={true},
  author={Huron, Samuel and Jansen, Yvonne and Carpendale, Sheelagh},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Constructing Visual Representations: Investigating the Use of Tangible Tokens}, 
  year={2014},
  volume={20},
  number={12},
  pages={2102-2111},
  keywords={Data visualization;Authoring tools;Image color analysis;Encoding;Publishing;Context awareness;Constructive visualization;Physical visualization;Dynamic visualization;Empirical study;Token;Visualization authoring;Information visualization;Visual mapping;Novices;Visualization construction;Visual analytics},
  doi={10.1109/TVCG.2014.2346292},
  video={https://www.youtube-nocookie.com/embed/_16WZgu85uw},
  preview={2014-constructing.png},
  selected={true},
  pdf={https://inria.hal.science/hal-01024053/document}
}

@inproceedings{ConstructiveVisualization,
  bibtex_show={true},
  author = {Huron, Samuel and Carpendale, Sheelagh and Thudt, Alice and Tang, Anthony and Mauerer, Michael},
  title = {Constructive visualization},
  year = {2014},
  isbn = {9781450329026},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2598510.2598566},
  doi = {10.1145/2598510.2598566},
  abstract = {If visualization is to be democratized, we need to provide means for non-experts to create visualizations that allow them to engage directly with datasets. We present constructive visualization a new paradigm for the simple creation of flexible, dynamic visualizations. Constructive visualization is simple-in that the skills required to build and manipulate the visualizations are akin to kindergarten play; it is expressive in that one can build within the constraints of the chosen environment, and it also supports dynamics -- in that these constructed visualizations can be rebuilt and adjusted. We de- scribe the conceptual components and processes underlying constructive visualization, and present real-world examples to illustrate the utility of this approach. The constructive visualization approach builds on our inherent understanding and experience with physical building blocks, offering a model that enables non-experts to create entirely novel visualizations, and to engage with datasets in a manner that would not have otherwise been possible.},
  booktitle = {Proceedings of the 2014 Conference on Designing Interactive Systems},
  pages = {433–442},
  numpages = {10},
  keywords = {visualization, visual literacy., education, design, constructivism, constructionism, construction, assembling},
  location = {Vancouver, BC, Canada},
  series = {DIS '14},
  preview={2014-constructive-vis.png},
  selected={true},
  pdf={https://inria.hal.science/hal-00978437/PDF/constructive-visualization-author-version.pdf}
}

@inproceedings{huron2013polemictweet,
  bibtex_show={true},
  title={PolemicTweet: Video annotation and analysis through tagged tweets},
  author={Huron, Samuel and Isenberg, Petra and Fekete, Jean Daniel},
  booktitle={Human-Computer Interaction--INTERACT 2013: 14th IFIP TC 13 International Conference, Cape Town, South Africa, September 2-6, 2013, Proceedings, Part II 14},
  abstract = {We present PolemicTweet a system with an encompassing, economic, and engaging approach to video tagging and analysis. Annotating and tagging videos manually is a boring and time-consuming process. Yet, in the last couple of years the audiences of events--such as academic conferences--have begun to produce unexploited metadata in the form of micropost activities. With PolemicTweet we explore the use of tagged microposts for both video an- notation and browsing aid. PolemicTweet is a system 1) to crowd source conference video tagging with structured sentiment metadata, 2) to engage audiences in a tagging process, and 3) to visualize these annotations for browsing and analyzing a video. We describe the the system and its components as well as the results from a one-year live deployment in 27 different events.},
  website = {https://polemictweet.com/},
  preview={2013-polemic-tweet.jpg},
  pdf={https://inria.hal.science/hal-00817591},
  pages={135--152},
  year={2013},
  annotation={* This paper received an Award},
  organization={Springer Berlin Heidelberg}
}

@inproceedings{mazieres2013toward,
  bibtex_show={true},
  title={Toward Google Borders},
  abstract ={Query logs let by user on search-engines have helped create efficient tools for trend analysis, from commercial use to forecasting epidemics. In this paper, we propose a new method and system for cultural trends analysis based on Google auto-complete suggestions. We present Zeitgeist Borders, a toolkit enabling any user to collect and analyze associations between queries, suggestions and various regions of the world. We report unexpected observations about several behavioural and geographical trends along with promising uses.},
  author={Mazieres, Antoine and Huron, Samuel},
  booktitle={Proceedings of the 5th Annual ACM Web Science Conference},
  preview={2013-google-borders.png},
  pages={244--247},
  year={2013}
}
